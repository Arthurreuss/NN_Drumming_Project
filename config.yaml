# ===============================
# Pipeline Configuration
# ===============================

pipeline:
  quantization: &q 16 #Options: [16, 32, 64]
  segment_len: &seg_len 128 #len of sample (At least 4x quantization to get one bar)
  model: large    #Options : [small, medium, large]
  create_dataset: False
  train_model: True
  inference: 
    enabled: False
    genre: hiphop #Options : [afrocuban, dance, funk, hiphop, jazz, latin, neworleans, pop, reggae, rock, soul] 
    generation_length: 128
    period: 4 #length of one bar
    temperature: 0.7
    plot: False # saves to outputs
    create_midi: False #saves to outputs
  evaluate: True

# ===============================
# Dataset Configuration
# ===============================

dataset_creation:
  seed: 42
  total_target: 240000 # total number of training samples
  raw_data_dir: dataset/e-gmd-v1.0.0 # path to raw midi files
  preprocessed_data_dir: dataset/processed # path to save processed data
  quantization: *q #Options: [16, 32, 64]
  segment_len: *seg_len #len of sample (At least 4x quantization to get one bar)
  train_test_val_split: [0.7, 0.2, 0.1] # train, test, val
  max_samples_per_file: 5  # max segments to extract from one midi file
  genres:
    - afrocuban
    - dance
    - funk
    - hiphop
    - jazz
    - latin
    - neworleans
    - pop
    - reggae
    - rock
    - soul
    - blues
  pitch_groups:
    kick: [35, 36]
    snare_center: [38, 40]
    snare_rim: [37]
    clap: [39]
    hh_closed: [42]
    hh_pedal: [44]
    hh_open: [46]
    tom_low: [41, 43]
    tom_mid: [45, 47]
    tom_high: [48, 50]
    crash: [49, 57]
    ride: [51, 59]
    ride_bell: [53]
  loudness_ranges: [0.0, 0.25, 0.5, 0.75, 1.0]
  keep_ratio: 0.95 #pruning

# ===============================
# Model Configuration
# ===============================

model:
  small:
    hidden_dim: 128
    num_layers: 1
    dropout: 0.2
    token_embed_dim: 64
    pos_embed_dim: 4  
    genre_embed_dim: 8
    bpm_embed_dim: 4
  medium:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.2
    token_embed_dim: 128
    pos_embed_dim: 8  
    genre_embed_dim: 8
    bpm_embed_dim: 4
  large:
    hidden_dim: 512
    num_layers: 3
    dropout: 0.3
    token_embed_dim: 256
    pos_embed_dim: 16  
    genre_embed_dim: 16
    bpm_embed_dim: 4

# ===============================
# Training Configuration
# ===============================

training:
  batch_size: 64
  epochs: 200
  learning_rate: 0.0005
  weight_decay: 0.0001
  teacher_forcing_start: 1.0
  teacher_forcing_end: -2.0
  early_stopping_patience: 5


