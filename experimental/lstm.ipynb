{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84b7a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No .npz files found in /Users/robsligter/Documents/Rug/Year 3/NN/NN_Drumming_Project/dataset/processed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m config_path = \u001b[33m\"\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m config_path = os.path.join(project_root, config_path)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dataset = \u001b[43mDrumDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_genre\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rug/Year 3/NN/NN_Drumming_Project/ml/data/dataset.py:18\u001b[39m, in \u001b[36mDrumDataset.__init__\u001b[39m\u001b[34m(self, data_dir, config_path, include_genre)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    data_dir (str): Directory containing .npz preprocessed files.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    config_path (str): Path to config.yaml containing list of genres.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    include_genre (bool): Whether to append one-hot genre encoding to X.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.files = \u001b[38;5;28mlist\u001b[39m(Path(data_dir).rglob(\u001b[33m\"\u001b[39m\u001b[33m*.npz\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.files) > \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo .npz files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.include_genre = include_genre\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mAssertionError\u001b[39m: No .npz files found in /Users/robsligter/Documents/Rug/Year 3/NN/NN_Drumming_Project/dataset/processed"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.dirname(os.curdir))\n",
    "project_root = os.path.dirname(project_root)\n",
    "\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from ml.data.dataset import DrumDataset\n",
    "\n",
    "data_dir = \"dataset/processed\"\n",
    "data_dir = os.path.join(project_root, data_dir)\n",
    "config_path = \"config.yaml\"\n",
    "config_path = os.path.join(project_root, config_path)\n",
    "\n",
    "dataset = DrumDataset(data_dir, config_path, include_genre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352685ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]\n",
    "print(\"Item shape:\", item[0].shape, item[1].shape)  #\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9187e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 63\n",
    "output_seq_len = 63\n",
    "input_size = 20\n",
    "output_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Load config to get seed\n",
    "with open(os.path.join(project_root, 'config.yaml'), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = config['dataset']['seed']\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"Using seed: {SEED} from config file\")\n",
    "\n",
    "# Training configuration\n",
    "DATASET_PERCENTAGE = 0.5  # Use 100% of dataset (change to 0.1 for 10%, 0.5 for 50%, etc.)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Check for device availability in order of preference: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=4, teacher_forcing_ratio=0.8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, x, y=None, target_len=1, training=False):\n",
    "        # x: (batch, input_seq_len, input_size)\n",
    "        # y: (batch, target_seq_len, output_size) ‚Äî only available during training\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "\n",
    "        # Encode input sequence\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "\n",
    "        # Start decoding with last known input\n",
    "        decoder_input = x[:, -1:, :]  # (batch, 1, input_size)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            pred = self.fc(out)  # (batch, 1, output_size)\n",
    "            outputs.append(pred)\n",
    "\n",
    "            # Decide if we use teacher forcing\n",
    "            if training and y is not None and random.random() < self.teacher_forcing_ratio:\n",
    "                decoder_input = y[:, t:t+1, :]  # use true value\n",
    "            else:\n",
    "                decoder_input = pred  # use predicted value\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # (batch, target_len, output_size)\n",
    "\n",
    "# Create subset of dataset based on percentage using config seed for reproducibility\n",
    "dataset_size = len(dataset)\n",
    "subset_size = int(dataset_size * DATASET_PERCENTAGE)\n",
    "\n",
    "# Use seeded random generator for consistent sampling\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "indices = torch.randperm(dataset_size, generator=generator)[:subset_size]\n",
    "subset_dataset = Subset(dataset, indices)\n",
    "\n",
    "print(f\"Original dataset size: {dataset_size}\")\n",
    "print(f\"Using {DATASET_PERCENTAGE*100}% of dataset: {subset_size} samples\")\n",
    "print(f\"Subset created with seed {SEED} for reproducible sampling\")\n",
    "\n",
    "dataloader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = Seq2SeqRNN(\n",
    "    input_size=20,\n",
    "    hidden_size=32,\n",
    "    output_size=20,\n",
    "    teacher_forcing_ratio=0.5\n",
    ").to(device)  # Move model to GPU/CPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "        # Move data to device\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X, Y, target_len=Y.shape[1], training=True)\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx}/{len(dataloader)}, Current Loss: {loss.item():.6f}\", end='\\r')\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} completed - Average Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913787e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 3, 1)\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss difference plot\n",
    "plt.subplot(1, 3, 2)\n",
    "loss_diff = [val - train for train, val in zip(train_losses, val_losses)]\n",
    "plt.plot(epochs_range, loss_diff, 'g-', label='Val - Train Loss', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Difference')\n",
    "plt.title('Validation - Training Loss Difference')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs_range, learning_rates, 'm-', label='Learning Rate', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Training Statistics:\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"Final Learning Rate: {learning_rates[-1]:.2e}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.6f} (Epoch {train_losses.index(min(train_losses)) + 1})\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.6f} (Epoch {val_losses.index(min(val_losses)) + 1})\")\n",
    "\n",
    "# Check for overfitting\n",
    "final_diff = val_losses[-1] - train_losses[-1]\n",
    "if final_diff > 0.1:\n",
    "    print(f\"‚ö†Ô∏è  Potential overfitting detected: Val loss is {final_diff:.6f} higher than train loss\")\n",
    "elif final_diff < -0.1:\n",
    "    print(f\"‚ö†Ô∏è  Unusual pattern: Val loss is {abs(final_diff):.6f} lower than train loss\")\n",
    "else:\n",
    "    print(f\"‚úÖ Good generalization: Val and train losses are close (diff: {final_diff:.6f})\")\n",
    "\n",
    "# Check learning rate reductions\n",
    "initial_lr = learning_rates[0]\n",
    "final_lr = learning_rates[-1]\n",
    "if final_lr < initial_lr:\n",
    "    reduction_factor = initial_lr / final_lr\n",
    "    print(f\"üìâ Learning rate was reduced by factor of {reduction_factor:.1f} during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from the validation set for testing\n",
    "val_sample_idx = 0  # First validation sample\n",
    "val_sample = val_dataset[val_sample_idx]\n",
    "X_sample = val_sample[0].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "Y_sample = val_sample[1].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "print(f\"Testing model on validation sample {val_sample_idx}\")\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    future_steps = model(X_sample, target_len=Y_sample.shape[1], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract data from tensors\n",
    "predicted = future_steps.squeeze(0).cpu().numpy()  # Remove batch dimension and convert to numpy\n",
    "actual = Y_sample.squeeze(0).cpu().numpy()  # Remove batch dimension and convert to numpy\n",
    "input_seq = X_sample.squeeze(0).cpu().numpy()  # Remove batch dimension and convert to numpy\n",
    "\n",
    "# Define instrument names (based on your dataset configuration)\n",
    "base_instruments = [\n",
    "    \"Kick\",\n",
    "    \"Snare\", \n",
    "    \"HH Closed\",\n",
    "    \"HH Open\",\n",
    "    \"Tom L\",\n",
    "    \"Tom M\", \n",
    "    \"Tom H\",\n",
    "    \"Crash\",\n",
    "    \"Ride\"\n",
    "]\n",
    "\n",
    "# Check if we have genre information (20 features total means 9 instruments + 11 genres)\n",
    "num_features = predicted.shape[1]\n",
    "if num_features > len(base_instruments) and hasattr(dataset, 'genres'):\n",
    "    instruments = base_instruments + dataset.genres\n",
    "else:\n",
    "    instruments = base_instruments[:num_features]  # Use only the available instruments\n",
    "\n",
    "# Create the comparison plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
    "fig.suptitle('LSTM Drum Pattern Prediction vs Actual', fontsize=16)\n",
    "\n",
    "# Plot input sequence\n",
    "im1 = axes[0].imshow(input_seq.T, aspect=\"auto\", origin=\"lower\", cmap=\"magma\", vmin=0, vmax=1)\n",
    "axes[0].set_yticks(range(len(instruments)))\n",
    "axes[0].set_yticklabels(instruments)\n",
    "axes[0].set_ylabel(\"Instruments\")\n",
    "axes[0].set_title(\"Input Sequence\")\n",
    "\n",
    "# Plot predicted sequence  \n",
    "im2 = axes[1].imshow(predicted.T, aspect=\"auto\", origin=\"lower\", cmap=\"magma\", vmin=0, vmax=1)\n",
    "axes[1].set_yticks(range(len(instruments)))\n",
    "axes[1].set_yticklabels(instruments)\n",
    "axes[1].set_ylabel(\"Instruments\")\n",
    "axes[1].set_title(\"Predicted Output\")\n",
    "\n",
    "# Plot actual sequence\n",
    "im3 = axes[2].imshow(actual.T, aspect=\"auto\", origin=\"lower\", cmap=\"magma\", vmin=0, vmax=1)\n",
    "axes[2].set_yticks(range(len(instruments)))\n",
    "axes[2].set_yticklabels(instruments)\n",
    "axes[2].set_xlabel(\"Timesteps\")\n",
    "axes[2].set_ylabel(\"Instruments\")\n",
    "axes[2].set_title(\"Actual Output\")\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im3, ax=axes, orientation='horizontal', pad=0.1, shrink=0.8, label='Activation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "mse = np.mean((predicted - actual) ** 2)\n",
    "mae = np.mean(np.abs(predicted - actual))\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Input sequence shape: {input_seq.shape}\")\n",
    "print(f\"Predicted sequence shape: {predicted.shape}\")\n",
    "print(f\"Actual sequence shape: {actual.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
